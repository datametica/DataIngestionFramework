tasks:
  - task_name: api-ingestion
    job_name: api-auth-ingestion
    input_processor: ApiProcessor
    job_prop_file: gs://bucket-name/property-dir/pubsub/job_prop.yaml
    api_path: 'https://www.boredapi.com/api/activity'
    query_params: query parameters
    root: DATA #optional tag. Used to get the key inside which we will get the actual record.
    authentication:
      is_public_api: true

    #if api in not public
    #authentication:
    #  headers: {"headers": "1"}
    targets:
      - bigquery:
          target_table: gcp_project.dataset_name.snwf_incremental
          write_disposition: write_append
#          write_method: STORAGE_WRITE_API  #optional - if not provided, DEFAULT will use STREAMING_INSERTS on Streaming pipelines and FILE_LOADS on Batch pipelines.

